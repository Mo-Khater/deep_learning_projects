# -*- coding: utf-8 -*-
"""torchtext.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l8LGSULP-fFQwgvYi0rsRhew7IC0-MAB
"""

from datasets import load_dataset
dataset = load_dataset("opus_books", "de-en")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
import spacy

nlp_en = spacy.load("en_core_web_sm")
nlp_de = spacy.load("de_core_news_sm")

from collections import Counter
class Vocab:
  def __init__(self,tokenizer,freq=1):
    self.tokenizer = tokenizer
    self.word2idx = {}
    self.idx2word = {}
    self.freq = freq

  def __len__(self):
    return len(self.word2idx)

  def build_vocab(self,sentence_list):
    tokenized_sentences = [self.tokenize(sentence) for sentence in sentence_list]
    frequencies = Counter()
    for tokenized_sentence in tokenized_sentences:
      frequencies.update(tokenized_sentence)

    self.word2idx = {'<unk>':0,'<pad>':1,'<sos>':2,'<eos>':3}
    self.idx2word = {0:'<unk>',1:'<pad>',2:'<sos>',3:'<eos>'}
    for word, freq in frequencies.items(): # Corrected iteration order
      if freq >= self.freq:
        id = len(self.word2idx)
        self.word2idx[word] = id
        self.idx2word[id] = word

  def tokenize(self,sentence):
    tokenized_sentence = self.tokenizer(sentence)
    return tokenized_sentence

  def encode(self,sentence):
    tokenized_sentence = self.tokenize(sentence)
    encoded_sentence = [self.word2idx[word] if word in self.word2idx else self.word2idx['<unk>'] for word in tokenized_sentence]
    return encoded_sentence

class MyCollate:
  def __init__(self,en_vocab,de_vocab):
    self.en_vocab = en_vocab
    self.de_vocab = de_vocab

  def __call__(self,batch):
    en_ids = [torch.tensor(self.en_vocab.encode(sentence['en'])) for sentence in batch]
    de_ids = [torch.tensor(self.de_vocab.encode(sentence['de'])) for sentence in batch]
    en_ids = pad_sequence(en_ids,batch_first=True,padding_value=self.en_vocab.word2idx['<pad>'])
    de_ids = pad_sequence(de_ids,batch_first=True,padding_value=self.de_vocab.word2idx['<pad>'])
    return en_ids,de_ids

en_vocab = Vocab(tokenizer=nlp_en,freq=1)
de_vocab = Vocab(tokenizer=nlp_de,freq=1)

# Build vocabularies using the training data from the loaded dataset
en_vocab.build_vocab([sentence['en'] for sentence in dataset['train']['translation']])
de_vocab.build_vocab([sentence['de'] for sentence in dataset['train']['translation']])

loader = DataLoader(dataset['train']['translation'], batch_size=32, shuffle=True,collate_fn=MyCollate(en_vocab,de_vocab))

class Encoder(nn.Module):
  def __init__(self,input_size,embedding_size,hidden_size,num_layers,p):
    super(Encoder,self).__init__()
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.dropout = nn.Dropout(p)
    self.embedding = nn.Embedding(input_size,embedding_size)
    self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,dropout=p,batch_first=True)

  def forward(self,x):
    print(x.shape)
    embedding = self.dropout(self.embedding(x))
    outputs,(hidden,cell) = self.rnn(embedding)
    return hidden,cell

class Decoder(nn.Module):
  def __init__(self,input_size,embedding_size,hidden_size,output_size,num_layers,p):
    super(Decoder,self).__init__()
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.dropout = nn.Dropout(p)
    self.embedding = nn.Embedding(input_size,embedding_size)
    self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,dropout=p,batch_first=True)
    self.fc = nn.Linear(hidden_size,output_size)

  def forward(self,x,hidden,cell):
    # x (batch_size)
    x = x.unsqueeze(1)
    # x (batch_size,1)
    embedding = self.dropout(self.embedding(x))
    # embedding (batch_size,1,emb_len)
    outputs,(hidden,cell) = self.rnn(embedding,(hidden,cell))
    # outputs (batch_size,1,hidden_size)
    predictions = self.fc(outputs)
    # predictions (batch_size,1,output_size)
    predictions = predictions.squeeze(1)
    return predictions,hidden,cell

import random
class Seq2Seq(nn.Module):
  def __init__(self,encoder,decoder):
    super(Seq2Seq,self).__init__()
    self.encoder = encoder
    self.decoder = decoder

  def forward(self,source,target,teacher_force_ratio=0.7):
    # source (batch_size,seq_len1)
    # target (batch_size,seq_len2)
    batch_size = source.shape[0]
    target_len = target.shape[1]
    target_vocab_size = len(en_vocab)
    outputs = torch.zeros(batch_size,target_len,target_vocab_size)
    hidden,cell = self.encoder(source)
    x = target[:,0]
    for t in range(1,target_len):
      output,hidden,cell = self.decoder(x,hidden,cell)
      outputs[:,t,:] = output
      best_guess = output.argmax(1)
      # best_guess (batch_size)
      x = target[:,t] if random.random() < teacher_force_ratio else best_guess
    return outputs

num_epochs = 10
batch_size = 32
hidden_size = 512
num_layers = 1
en_vocab_len = len(en_vocab)
de_vocab_len = len(de_vocab)
embedding_size = 300
learning_rate = 0.001
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
enc = Encoder(de_vocab_len,embedding_size,hidden_size,num_layers,p=0.5).to(device)
dec = Decoder(en_vocab_len,embedding_size,hidden_size,en_vocab_len,num_layers,p=0.5).to(device)
model = Seq2Seq(enc,dec).to(device)
optimizer = optim.Adam(model.parameters(),lr=learning_rate)
criterion = nn.CrossEntropyLoss(ignore_index=1)

def translate_sentence(model, sentence, de_vocab, en_vocab, device, max_length=50):
    # print(sentence)

    # sys.exit()

    # Load german tokenizer
    spacy_ger = spacy.load("de_core_news_sm")

    # Create tokens using spacy and everything in lower case (which is what our vocab is)
    if type(sentence) == str:
        tokens = [token.text.lower() for token in spacy_ger(sentence)]
    else:
        tokens = [token.lower() for token in sentence]

    # print(tokens)

    # sys.exit()
    # Add <SOS> and <EOS> in beginning and end respectively
    tokens.insert(0, '<sos>')
    tokens.append('<eos>')

    # Go through each german token and convert to an index
    text_to_indices = [de_vocab.word2idx[token] if token in de_vocab.word2idx else de_vocab.word2idx['<unk>'] for token in tokens ]

    # Convert to Tensor
    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(0).to(device)

    # Build encoder hidden, cell state
    with torch.no_grad():
        hidden, cell = model.encoder(sentence_tensor)
    print(hidden.shape)
    outputs = [en_vocab.word2idx["<sos>"]]

    for _ in range(max_length):
        previous_word = torch.LongTensor([outputs[-1]]).to(device)

        with torch.no_grad():
            output, hidden, cell = model.decoder(previous_word, hidden, cell)
            best_guess = output.argmax(1).item()

        outputs.append(best_guess)

        # Model predicts it's the end of the sentence
        if output.argmax(1).item() == en_vocab.word2idx["<eos>"]:
            break

    translated_sentence = [en_vocab.itos[idx] for idx in outputs]

    # remove start token
    return translated_sentence[1:]

for epoch in range(num_epochs):
  for batch_idx, (input, target) in enumerate(loader):
    # input (batch_size,en_vocab_len)
    # target (batch_size,de_vocab_len)
    translate_sentence(model,'Ich lerne jeden Tag etwas Neues mit Python.',de_vocab,en_vocab,device)
    input = input.to(device)
    target = target.to(device)
    output = model(input,target)
    # output (batch_size,target_len,en_vocab_size)
    output = output[:,1:,:].reshape(-1,output.shape[2])
    target = target[:,1:].reshape(-1)
    optimizer.zero_grad()
    loss = criterion(output,target)
    loss.backward()
    optimizer.step()

